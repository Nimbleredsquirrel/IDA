{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "047674ee",
   "metadata": {},
   "source": [
    "# Домашняя работа "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8461109",
   "metadata": {},
   "source": [
    "В этой работы мы будем учиться предсказывать зарплату data scientist-ов в зависимочти от ряда факторов с помощью градиентоного бустинга"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2ab23d",
   "metadata": {},
   "source": [
    "Цель домашней работы научиться предсказывать зарплаты (salary_in_usd) по ряды факторов "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1651e8dd",
   "metadata": {},
   "source": [
    "work_year: The number of years of work experience in the field of data science.\n",
    "\n",
    "experience_level: The level of experience, such as Junior, Senior, or Lead.\n",
    "\n",
    "employment_type: The type of employment, such as Full-time or Contract.\n",
    "\n",
    "job_title: The specific job title or role, such as Data Analyst or Data Scientist.\n",
    "\n",
    "salary: The salary amount for the given job.\n",
    "\n",
    "salary_currency: The currency in which the salary is denoted.\n",
    "\n",
    "salary_in_usd: The equivalent salary amount converted to US dollars (USD) for comparison purposes.\n",
    "\n",
    "employee_residence: The country or region where the employee resides.\n",
    "\n",
    "remote_ratio: The percentage of remote work offered in the job.\n",
    "\n",
    "company_location: The location of the company or organization.\n",
    "\n",
    "company_size: The company’s size is categorized as Small, Medium, or Large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbf7595c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "792d14c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"ds_salaries.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1758b0e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>work_year</th>\n",
       "      <th>experience_level</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>job_title</th>\n",
       "      <th>salary</th>\n",
       "      <th>salary_currency</th>\n",
       "      <th>salary_in_usd</th>\n",
       "      <th>employee_residence</th>\n",
       "      <th>remote_ratio</th>\n",
       "      <th>company_location</th>\n",
       "      <th>company_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Principal Data Scientist</td>\n",
       "      <td>80000</td>\n",
       "      <td>EUR</td>\n",
       "      <td>85847</td>\n",
       "      <td>ES</td>\n",
       "      <td>100</td>\n",
       "      <td>ES</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023</td>\n",
       "      <td>MI</td>\n",
       "      <td>CT</td>\n",
       "      <td>ML Engineer</td>\n",
       "      <td>30000</td>\n",
       "      <td>USD</td>\n",
       "      <td>30000</td>\n",
       "      <td>US</td>\n",
       "      <td>100</td>\n",
       "      <td>US</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023</td>\n",
       "      <td>MI</td>\n",
       "      <td>CT</td>\n",
       "      <td>ML Engineer</td>\n",
       "      <td>25500</td>\n",
       "      <td>USD</td>\n",
       "      <td>25500</td>\n",
       "      <td>US</td>\n",
       "      <td>100</td>\n",
       "      <td>US</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>175000</td>\n",
       "      <td>USD</td>\n",
       "      <td>175000</td>\n",
       "      <td>CA</td>\n",
       "      <td>100</td>\n",
       "      <td>CA</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>120000</td>\n",
       "      <td>USD</td>\n",
       "      <td>120000</td>\n",
       "      <td>CA</td>\n",
       "      <td>100</td>\n",
       "      <td>CA</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   work_year experience_level employment_type                 job_title  \\\n",
       "0       2023               SE              FT  Principal Data Scientist   \n",
       "1       2023               MI              CT               ML Engineer   \n",
       "2       2023               MI              CT               ML Engineer   \n",
       "3       2023               SE              FT            Data Scientist   \n",
       "4       2023               SE              FT            Data Scientist   \n",
       "\n",
       "   salary salary_currency  salary_in_usd employee_residence  remote_ratio  \\\n",
       "0   80000             EUR          85847                 ES           100   \n",
       "1   30000             USD          30000                 US           100   \n",
       "2   25500             USD          25500                 US           100   \n",
       "3  175000             USD         175000                 CA           100   \n",
       "4  120000             USD         120000                 CA           100   \n",
       "\n",
       "  company_location company_size  \n",
       "0               ES            L  \n",
       "1               US            S  \n",
       "2               US            S  \n",
       "3               CA            M  \n",
       "4               CA            M  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6696e77a",
   "metadata": {},
   "source": [
    "## Задание 1 (0.5 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb13dc7",
   "metadata": {},
   "source": [
    "Разделите выборку на train, val, test (80%, 10%, 10%) в качестве таргета выберите salary_in_usd, удалите из признаков (salary) чтобы избежать лика в данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "c7015bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "\n",
    "y = df.salary_in_usd\n",
    "X = df.drop(['salary', 'salary_in_usd'], axis= 1)\n",
    "X_train, X_test_val, y_train, y_test_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=123\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_test_val, y_test_val, test_size=0.5, random_state=123\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63cb01a",
   "metadata": {},
   "source": [
    "## Задание 2 (0.5 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647a5cad",
   "metadata": {},
   "source": [
    "Обучите модель линейной регресии и оцените её качество через mape и rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9c534377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 3.7866249129416595e+17\n",
      "Test RMSE: 6.023480300609078e+17\n",
      "Validation MAPE: 2233289783216.386\n",
      "Test MAPE: 659137930373.1843\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error\n",
    "\n",
    "cat_features_train = X_train.dtypes == \"object\"\n",
    "transformer = make_column_transformer(\n",
    "    (OneHotEncoder(handle_unknown = 'ignore'), cat_features_train),\n",
    "    remainder='passthrough')\n",
    "transformed_train = transformer.fit_transform(X_train).toarray()\n",
    "X_train_l = pd.DataFrame(\n",
    "    transformed_train, \n",
    "    columns=transformer.get_feature_names_out())\n",
    "\n",
    "cat_features_val = X_val.dtypes == \"object\"\n",
    "\n",
    "transformed_val = transformer.transform(X_val).toarray()\n",
    "X_val_l = pd.DataFrame(\n",
    "    transformed_val, \n",
    "    columns=transformer.get_feature_names_out())\n",
    "\n",
    "cat_features_test = X_test.dtypes == \"object\"\n",
    "transformed_test = transformer.transform(X_test).toarray()\n",
    "X_test_l = pd.DataFrame(\n",
    "    transformed_test, \n",
    "    columns=transformer.get_feature_names_out())\n",
    "\n",
    "ss = StandardScaler()\n",
    "X_train_l = ss.fit_transform(X_train_l)\n",
    "X_val_l = ss.transform(X_val_l)\n",
    "X_test_l = ss.transform(X_test_l)\n",
    "\n",
    "lr = LinearRegression().fit(X_train_l, y_train)\n",
    "print(f\"Validation RMSE: {mean_squared_error(y_val, lr.predict(X_val_l), squared=False)}\")\n",
    "print(f\"Test RMSE: {mean_squared_error(y_test, lr.predict(X_test_l), squared=False)}\")\n",
    "print(f\"Validation MAPE: {mean_absolute_percentage_error(y_val, lr.predict(X_val_l))}\")\n",
    "print(f\"Test MAPE: {mean_absolute_percentage_error(y_test, lr.predict(X_test_l))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf8374f",
   "metadata": {},
   "source": [
    "## Задание 3 (0.5 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97440d17",
   "metadata": {},
   "source": [
    "Обучите модель бустинга выберите любую из трех библиотек catboost, xgboost, lightgbm и оцените её качество через mape и rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4dd05a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045913 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 90\n",
      "[LightGBM] [Info] Number of data points in the train set: 3004, number of used features: 43\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Start training from score 138580.205393\n",
      "[1]\tvalid_0's l2: 3.38447e+09\n",
      "[2]\tvalid_0's l2: 3.09522e+09\n",
      "[3]\tvalid_0's l2: 2.86948e+09\n",
      "[4]\tvalid_0's l2: 2.68197e+09\n",
      "[5]\tvalid_0's l2: 2.52952e+09\n",
      "[6]\tvalid_0's l2: 2.40706e+09\n",
      "[7]\tvalid_0's l2: 2.31077e+09\n",
      "[8]\tvalid_0's l2: 2.22406e+09\n",
      "[9]\tvalid_0's l2: 2.16576e+09\n",
      "[10]\tvalid_0's l2: 2.11226e+09\n",
      "Validation RMSE: 45959.38206922794\n",
      "Test RMSE: 46869.179348244725\n",
      "Validation MAPE: 0.5223231060550186\n",
      "Test MAPE: 0.43817998656238916\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import sklearn\n",
    "\n",
    "cat_features_train = X_train.dtypes == \"object\"\n",
    "transformer = make_column_transformer(\n",
    "    (OneHotEncoder(handle_unknown = 'ignore'), cat_features_train),\n",
    "    remainder='passthrough')\n",
    "transformed_train = transformer.fit_transform(X_train).toarray()\n",
    "X_train_l = pd.DataFrame(\n",
    "    transformed_train, \n",
    "    columns=transformer.get_feature_names_out())\n",
    "\n",
    "cat_features_val = X_val.dtypes == \"object\"\n",
    "\n",
    "transformed_val = transformer.transform(X_val).toarray()\n",
    "X_val_l = pd.DataFrame(\n",
    "    transformed_val, \n",
    "    columns=transformer.get_feature_names_out())\n",
    "\n",
    "cat_features_test = X_test.dtypes == \"object\"\n",
    "transformed_test = transformer.transform(X_test).toarray()\n",
    "X_test_l = pd.DataFrame(\n",
    "    transformed_test, \n",
    "    columns=transformer.get_feature_names_out())\n",
    "\n",
    "train_dataset = lgb.Dataset(X_train_l, y_train, feature_name=X_train_l.columns.values.tolist())\n",
    "val_dataset = lgb.Dataset(X_val_l, y_val, feature_name=X_val_l.columns.values.tolist())\n",
    "test_dataset = lgb.Dataset(X_test_l, y_test, feature_name=X_test_l.columns.values.tolist())\n",
    "\n",
    "booster = lgb.train({\"objective\": \"regression\"},\n",
    "                    train_set=train_dataset, valid_sets=(val_dataset,),\n",
    "                    num_boost_round=10)\n",
    "\n",
    "train_preds = booster.predict(X_train_l)\n",
    "val_preds = booster.predict(X_val_l)\n",
    "test_preds = booster.predict(X_test_l)\n",
    "\n",
    "print(f\"Validation RMSE: {mean_squared_error(y_val, val_preds, squared=False)}\")\n",
    "print(f\"Test RMSE: {mean_squared_error(y_test, test_preds, squared=False)}\")\n",
    "print(f\"Validation MAPE: {mean_absolute_percentage_error(y_val, val_preds)}\")\n",
    "print(f\"Test MAPE: {mean_absolute_percentage_error(y_test, test_preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b76f89",
   "metadata": {},
   "source": [
    "## Задание 4 (0.5 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34452854",
   "metadata": {},
   "source": [
    "Далее предобработайте категориальные признаки с помощью onehot кодирования, labelencoder и счетчиков (mean target encoding) сравните качество моделей с помощью mape и rmse. Необходимо обучить две модели линейную и бустинг. Какие выводы можно сделать? Выберите лучшую из моделей и дальше будем работать с ней"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "a3a4b9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_95/1972532779.py:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_val = X_train.append(X_val)\n"
     ]
    }
   ],
   "source": [
    "# one-hot encoding\n",
    "cat_features_train = X_train.dtypes == \"object\"\n",
    "transformer = make_column_transformer(\n",
    "    (OneHotEncoder(handle_unknown = 'ignore'), cat_features_train[:3]),\n",
    "    remainder='passthrough')\n",
    "transformed_train = transformer.fit_transform(X_train[[\"work_year\", \"experience_level\", \"employment_type\"]])\n",
    "X_train_l = pd.DataFrame(\n",
    "    transformed_train, \n",
    "    columns=transformer.get_feature_names_out())\n",
    "\n",
    "transformed_val = transformer.transform(X_val[[\"work_year\", \"experience_level\", \"employment_type\"]])\n",
    "X_val_l = pd.DataFrame(\n",
    "    transformed_val, \n",
    "    columns=transformer.get_feature_names_out())\n",
    "\n",
    "transformed_test = transformer.transform(X_test[[\"work_year\", \"experience_level\", \"employment_type\"]])\n",
    "X_test_l = pd.DataFrame(\n",
    "    transformed_test, \n",
    "    columns=transformer.get_feature_names_out())\n",
    "\n",
    "#labelencoder\n",
    "from sklearn import preprocessing\n",
    "cat_features= [\"salary_currency\", \"company_size\"]\n",
    "train_val = X_train.append(X_val)\n",
    "encoders = dict()\n",
    "\n",
    "for cat in cat_features:\n",
    "    encoders[cat] = preprocessing.LabelEncoder()\n",
    "    train_val[cat] = encoders[cat].fit(train_val[cat])\n",
    "    X_train_l[cat] = encoders[cat].transform(X_train[cat])\n",
    "    X_val_l[cat] = encoders[cat].transform(X_val[cat])\n",
    "    X_test_l[cat] = encoders[cat].transform(X_test[cat])\n",
    "\n",
    "#mean target encoding    \n",
    "import category_encoders as ce\n",
    "\n",
    "encoder = ce.TargetEncoder()\n",
    "X_train_new = encoder.fit_transform(X_train[[\"job_title\", \"employee_residence\"]], y_train)\n",
    "X_val_new = encoder.transform(X_val[[\"job_title\", \"employee_residence\"]])\n",
    "X_test_new = encoder.transform(X_test[[\"job_title\", \"employee_residence\"]])\n",
    "\n",
    "X_train_l = pd.concat([X_train_l, X_train_new.reset_index(drop=True)], axis=1)\n",
    "X_val_l = pd.concat([X_val_l, X_val_new.reset_index(drop=True)], axis=1)\n",
    "X_test_l = pd.concat([X_test_l, X_test_new.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "8bdd307c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE lr: 44498.913242987146\n",
      "Test RMSE lr: 46380.28748273\n",
      "Validation MAPE lr: 0.4301482909298643\n",
      "Test MAPE lr: 0.36652514925802804\n"
     ]
    }
   ],
   "source": [
    "#Linear regression\n",
    "ss = StandardScaler()\n",
    "X_train_lr = ss.fit_transform(X_train_l)\n",
    "X_val_lr = ss.transform(X_val_l)\n",
    "X_test_lr = ss.transform(X_test_l)\n",
    "\n",
    "lr = LinearRegression().fit(X_train_l, y_train)\n",
    "print(f\"Validation RMSE lr: {mean_squared_error(y_val, lr.predict(X_val_l), squared=False)}\")\n",
    "print(f\"Test RMSE lr: {mean_squared_error(y_test, lr.predict(X_test_l), squared=False)}\")\n",
    "print(f\"Validation MAPE lr: {mean_absolute_percentage_error(y_val, lr.predict(X_val_l))}\")\n",
    "print(f\"Test MAPE lr: {mean_absolute_percentage_error(y_test, lr.predict(X_test_l))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "7aad238c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016203 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 142\n",
      "[LightGBM] [Info] Number of data points in the train set: 3004, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 138580.205393\n",
      "[1]\tvalid_0's l2: 3.4028e+09\n",
      "[2]\tvalid_0's l2: 3.13168e+09\n",
      "[3]\tvalid_0's l2: 2.91194e+09\n",
      "[4]\tvalid_0's l2: 2.74337e+09\n",
      "[5]\tvalid_0's l2: 2.60255e+09\n",
      "[6]\tvalid_0's l2: 2.48527e+09\n",
      "[7]\tvalid_0's l2: 2.37995e+09\n",
      "[8]\tvalid_0's l2: 2.29298e+09\n",
      "[9]\tvalid_0's l2: 2.22004e+09\n",
      "[10]\tvalid_0's l2: 2.16605e+09\n",
      "Validation RMSE lightgbm: 46540.89003056658\n",
      "Test RMSE lightgbm: 47186.01613261466\n",
      "Validation MAPE lightgbm: 0.5546917011622176\n",
      "Test MAPE lightgbm: 0.44561617175276635\n"
     ]
    }
   ],
   "source": [
    "#lightgbm\n",
    "train_dataset = lgb.Dataset(X_train_l, y_train, feature_name=X_train_l.columns.values.tolist())\n",
    "val_dataset = lgb.Dataset(X_val_l, y_val, feature_name=X_val_l.columns.values.tolist())\n",
    "test_dataset = lgb.Dataset(X_test_l, y_test, feature_name=X_test_l.columns.values.tolist())\n",
    "\n",
    "booster = lgb.train({\"objective\": \"regression\"},\n",
    "                    train_set=train_dataset, valid_sets=(val_dataset,),\n",
    "                    num_boost_round=10)\n",
    "\n",
    "train_preds = booster.predict(X_train_l)\n",
    "val_preds = booster.predict(X_val_l)\n",
    "test_preds = booster.predict(X_test_l)\n",
    "\n",
    "print(f\"Validation RMSE lightgbm: {mean_squared_error(y_val, val_preds, squared=False)}\")\n",
    "print(f\"Test RMSE lightgbm: {mean_squared_error(y_test, test_preds, squared=False)}\")\n",
    "print(f\"Validation MAPE lightgbm: {mean_absolute_percentage_error(y_val, val_preds)}\")\n",
    "print(f\"Test MAPE lightgbm: {mean_absolute_percentage_error(y_test, test_preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9d6d50",
   "metadata": {},
   "source": [
    "### Ответ:\n",
    "Линейная регрессия справилась лучше, так как для lightgbm нужно я не подбирала гиперпараметры, чтобы повысить точность. \n",
    "Вообще один из выводов - это то, что удобнее всего использовать TargetEncoder, потому что иначе может появиться ошибка, если в тренировочных данных не было той категории, которая была в тестовых данных. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119b89f1",
   "metadata": {},
   "source": [
    "## Задание 5 (0.5 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140b8af1",
   "metadata": {},
   "source": [
    "Покажите, где ошибается ваша модель. Выведите топ 20 примеров с наибольшей ошибкой. Проанализируете их, постарайтесь выделить группы с причинами ошибок в моделе. Какие выводы можно сделать? Что стоит изменить в данных чтобы улучшить качество модели?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "ef2a8666",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new = pd.DataFrame(y_test)\n",
    "y_new['predictions'] = test_preds\n",
    "y_new['difference'] = abs(y_new['predictions'] - y_new['salary_in_usd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "468d199f",
   "metadata": {},
   "outputs": [],
   "source": [
    "the_highest_error = y_new.sort_values('difference', ascending=False)[:20].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "9a9512db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>work_year</th>\n",
       "      <th>experience_level</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>job_title</th>\n",
       "      <th>salary_currency</th>\n",
       "      <th>employee_residence</th>\n",
       "      <th>remote_ratio</th>\n",
       "      <th>company_location</th>\n",
       "      <th>company_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2374</th>\n",
       "      <td>2022</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>USD</td>\n",
       "      <td>US</td>\n",
       "      <td>100</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2023</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Computer Vision Engineer</td>\n",
       "      <td>USD</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1286</th>\n",
       "      <td>2023</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Machine Learning Engineer</td>\n",
       "      <td>USD</td>\n",
       "      <td>US</td>\n",
       "      <td>100</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>2023</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>USD</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>2023</td>\n",
       "      <td>MI</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>USD</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1185</th>\n",
       "      <td>2023</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Machine Learning Engineer</td>\n",
       "      <td>USD</td>\n",
       "      <td>US</td>\n",
       "      <td>100</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2986</th>\n",
       "      <td>2022</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>ETL Developer</td>\n",
       "      <td>USD</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>2023</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>USD</td>\n",
       "      <td>US</td>\n",
       "      <td>100</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2595</th>\n",
       "      <td>2022</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>USD</td>\n",
       "      <td>US</td>\n",
       "      <td>100</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2591</th>\n",
       "      <td>2022</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>USD</td>\n",
       "      <td>US</td>\n",
       "      <td>100</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>2023</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>USD</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3455</th>\n",
       "      <td>2020</td>\n",
       "      <td>MI</td>\n",
       "      <td>FT</td>\n",
       "      <td>Product Data Analyst</td>\n",
       "      <td>USD</td>\n",
       "      <td>HN</td>\n",
       "      <td>0</td>\n",
       "      <td>HN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2962</th>\n",
       "      <td>2022</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>USD</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2601</th>\n",
       "      <td>2022</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>USD</td>\n",
       "      <td>US</td>\n",
       "      <td>100</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1846</th>\n",
       "      <td>2022</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>USD</td>\n",
       "      <td>US</td>\n",
       "      <td>100</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2430</th>\n",
       "      <td>2022</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>USD</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1609</th>\n",
       "      <td>2023</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>USD</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3701</th>\n",
       "      <td>2021</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Research Scientist</td>\n",
       "      <td>USD</td>\n",
       "      <td>FR</td>\n",
       "      <td>100</td>\n",
       "      <td>US</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3037</th>\n",
       "      <td>2022</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Architect</td>\n",
       "      <td>USD</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2835</th>\n",
       "      <td>2022</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>USD</td>\n",
       "      <td>ES</td>\n",
       "      <td>100</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      work_year experience_level employment_type                  job_title  \\\n",
       "2374       2022               SE              FT             Data Scientist   \n",
       "33         2023               SE              FT   Computer Vision Engineer   \n",
       "1286       2023               SE              FT  Machine Learning Engineer   \n",
       "1458       2023               SE              FT              Data Engineer   \n",
       "841        2023               MI              FT              Data Engineer   \n",
       "1185       2023               SE              FT  Machine Learning Engineer   \n",
       "2986       2022               SE              FT              ETL Developer   \n",
       "565        2023               SE              FT             Data Scientist   \n",
       "2595       2022               SE              FT             Data Scientist   \n",
       "2591       2022               SE              FT             Data Scientist   \n",
       "1066       2023               SE              FT              Data Engineer   \n",
       "3455       2020               MI              FT       Product Data Analyst   \n",
       "2962       2022               SE              FT              Data Engineer   \n",
       "2601       2022               SE              FT             Data Scientist   \n",
       "1846       2022               SE              FT             Data Scientist   \n",
       "2430       2022               SE              FT              Data Engineer   \n",
       "1609       2023               SE              FT              Data Engineer   \n",
       "3701       2021               SE              FT         Research Scientist   \n",
       "3037       2022               SE              FT             Data Architect   \n",
       "2835       2022               SE              FT              Data Engineer   \n",
       "\n",
       "     salary_currency employee_residence  remote_ratio company_location  \\\n",
       "2374             USD                 US           100               US   \n",
       "33               USD                 US             0               US   \n",
       "1286             USD                 US           100               US   \n",
       "1458             USD                 US             0               US   \n",
       "841              USD                 US             0               US   \n",
       "1185             USD                 US           100               US   \n",
       "2986             USD                 US             0               US   \n",
       "565              USD                 US           100               US   \n",
       "2595             USD                 US           100               US   \n",
       "2591             USD                 US           100               US   \n",
       "1066             USD                 US             0               US   \n",
       "3455             USD                 HN             0               HN   \n",
       "2962             USD                 US             0               US   \n",
       "2601             USD                 US           100               US   \n",
       "1846             USD                 US           100               US   \n",
       "2430             USD                 US             0               US   \n",
       "1609             USD                 US             0               US   \n",
       "3701             USD                 FR           100               US   \n",
       "3037             USD                 US             0               US   \n",
       "2835             USD                 ES           100               US   \n",
       "\n",
       "     company_size  \n",
       "2374            M  \n",
       "33              M  \n",
       "1286            M  \n",
       "1458            M  \n",
       "841             M  \n",
       "1185            M  \n",
       "2986            M  \n",
       "565             M  \n",
       "2595            M  \n",
       "2591            M  \n",
       "1066            M  \n",
       "3455            S  \n",
       "2962            M  \n",
       "2601            M  \n",
       "1846            M  \n",
       "2430            M  \n",
       "1609            M  \n",
       "3701            S  \n",
       "3037            M  \n",
       "2835            M  "
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.loc[the_highest_error]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7a7d28",
   "metadata": {},
   "source": [
    "Я проверила, что категории \"HN\" нет в тренировочных и валидационных данных, это могло послужить одной из причин высокого значения ошибки. Однако по остальным параметрам сложно сделать вывод, почему ошибка оказалась большой."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb42f2e",
   "metadata": {},
   "source": [
    "## Задание 6 (0.5 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348b42f3",
   "metadata": {},
   "source": [
    "Придумайте признаки для улучшения качества модели на основе предыдущего пункта. Как вам кажется какими признаками можно улучшить качетсво модели? Реализуйте признаки и проверьте улучшилось ли качество модели. Полный бал за задание ставится даже при отсутствии улучшения качества модели, важно попробовать проверить свои гипотезы и сделать выводы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e268e771",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3578292c",
   "metadata": {},
   "source": [
    "## Задание 7 (0.5 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1ce455",
   "metadata": {},
   "source": [
    "Теперь используете исключительно catboost. Обучить модель. Подберите оптимальные гиперпараметры. Используйте pool для передачи данных в модель с указанием какие признаки категориальные, а какие нет с помощью параметры cat_features. Оцените качество итоговой модели, скорость обучения и скорость предсказания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a34a3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3bfc4e05",
   "metadata": {},
   "source": [
    "## Задание 8 (0.5 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14466af9",
   "metadata": {},
   "source": [
    "Теперь используете исключительно xgboost. Обучить модель. Подберите оптимальные гиперпараметры. Закодируйте категориальные переменные, как хотите. Оцените качество итоговой модели, скорость обучения и скорость предсказания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c089e017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-1.7.6-py3-none-manylinux2014_x86_64.whl (200.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.3/200.3 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy in /home/arinkazam/.local/lib/python3.10/site-packages (from xgboost) (1.9.3)\n",
      "Requirement already satisfied: numpy in /home/arinkazam/.local/lib/python3.10/site-packages (from xgboost) (1.23.5)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.7.6\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "62a6b1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 15.5 µs\n",
      "66517.4501198661\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 9.54 µs\n",
      "Validation RMSE: 66517.44996999443\n",
      "Test RMSE: 67804.26649888395\n",
      "Validation MAPE: 0.39997019887202667\n",
      "Test MAPE: 0.3773581608845906\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"n_estimators\":100,\n",
    "    \"max_depth\": 4,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.9,\n",
    "    \"colsample_bylevel\": 0.8,\n",
    "    \"reg_lambda\": 0.1,\n",
    "    \"eval_metric\": \"rmse\",\n",
    "    \"random_state\": 42,\n",
    "}\n",
    "\n",
    "reg = xgb.XGBRegressor(**params)\n",
    "\n",
    "%time\n",
    "reg.fit(X_train_l, \n",
    "        y_train, \n",
    "        verbose=False,\n",
    "        eval_set= [(X_train_l, y_train), (X_val_l, y_val)],\n",
    "        early_stopping_rounds= 3\n",
    "        )\n",
    "results = reg.evals_result()\n",
    "best_iter = reg.best_iteration\n",
    "best_iter\n",
    "print(results['validation_1']['rmse'][best_iter])\n",
    "\n",
    "%time\n",
    "test_preds = reg.predict(X_test_l)\n",
    "val_preds = reg.predict(X_val_l)\n",
    "print(f\"Validation RMSE: {mean_squared_error(y_val, val_preds, squared=False)}\")\n",
    "print(f\"Test RMSE: {mean_squared_error(y_test, test_preds, squared=False)}\")\n",
    "print(f\"Validation MAPE: {mean_absolute_percentage_error(y_val, val_preds)}\")\n",
    "print(f\"Test MAPE: {mean_absolute_percentage_error(y_test, test_preds)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a881ad9d",
   "metadata": {},
   "source": [
    "## Задание 9 (0.5 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35f87a6",
   "metadata": {},
   "source": [
    "Теперь используете исключительно lightgbm. Обучить модель. Подберите оптимальные гиперпараметры. Закодируйте категориальные переменные, как хотите. Оцените качество итоговой модели, скорость обучения и скорость предсказания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6876f682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 8.11 µs\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000357 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 90\n",
      "[LightGBM] [Info] Number of data points in the train set: 3004, number of used features: 43\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Start training from score 138580.205393\n",
      "[1]\tvalid_0's l2: 3.38447e+09\n",
      "[2]\tvalid_0's l2: 3.09522e+09\n",
      "[3]\tvalid_0's l2: 2.86948e+09\n",
      "[4]\tvalid_0's l2: 2.68197e+09\n",
      "[5]\tvalid_0's l2: 2.52952e+09\n",
      "[6]\tvalid_0's l2: 2.40706e+09\n",
      "[7]\tvalid_0's l2: 2.31077e+09\n",
      "[8]\tvalid_0's l2: 2.22406e+09\n",
      "[9]\tvalid_0's l2: 2.16576e+09\n",
      "[10]\tvalid_0's l2: 2.11226e+09\n",
      "[11]\tvalid_0's l2: 2.0671e+09\n",
      "[12]\tvalid_0's l2: 2.03071e+09\n",
      "[13]\tvalid_0's l2: 2.00102e+09\n",
      "[14]\tvalid_0's l2: 1.97274e+09\n",
      "[15]\tvalid_0's l2: 1.95018e+09\n",
      "[16]\tvalid_0's l2: 1.93611e+09\n",
      "[17]\tvalid_0's l2: 1.9154e+09\n",
      "[18]\tvalid_0's l2: 1.89981e+09\n",
      "[19]\tvalid_0's l2: 1.88603e+09\n",
      "[20]\tvalid_0's l2: 1.87484e+09\n",
      "[21]\tvalid_0's l2: 1.86304e+09\n",
      "[22]\tvalid_0's l2: 1.85475e+09\n",
      "[23]\tvalid_0's l2: 1.8482e+09\n",
      "[24]\tvalid_0's l2: 1.84261e+09\n",
      "[25]\tvalid_0's l2: 1.83799e+09\n",
      "[26]\tvalid_0's l2: 1.83475e+09\n",
      "[27]\tvalid_0's l2: 1.83313e+09\n",
      "[28]\tvalid_0's l2: 1.82934e+09\n",
      "[29]\tvalid_0's l2: 1.83158e+09\n",
      "[30]\tvalid_0's l2: 1.83126e+09\n",
      "[31]\tvalid_0's l2: 1.82966e+09\n",
      "[32]\tvalid_0's l2: 1.8286e+09\n",
      "[33]\tvalid_0's l2: 1.83119e+09\n",
      "[34]\tvalid_0's l2: 1.83056e+09\n",
      "[35]\tvalid_0's l2: 1.82932e+09\n",
      "[36]\tvalid_0's l2: 1.82878e+09\n",
      "[37]\tvalid_0's l2: 1.82801e+09\n",
      "[38]\tvalid_0's l2: 1.82795e+09\n",
      "[39]\tvalid_0's l2: 1.82941e+09\n",
      "[40]\tvalid_0's l2: 1.83388e+09\n",
      "[41]\tvalid_0's l2: 1.83486e+09\n",
      "[42]\tvalid_0's l2: 1.83572e+09\n",
      "[43]\tvalid_0's l2: 1.83522e+09\n",
      "[44]\tvalid_0's l2: 1.83751e+09\n",
      "[45]\tvalid_0's l2: 1.83895e+09\n",
      "[46]\tvalid_0's l2: 1.84125e+09\n",
      "[47]\tvalid_0's l2: 1.84275e+09\n",
      "[48]\tvalid_0's l2: 1.84574e+09\n",
      "[49]\tvalid_0's l2: 1.84482e+09\n",
      "[50]\tvalid_0's l2: 1.84634e+09\n",
      "[51]\tvalid_0's l2: 1.84991e+09\n",
      "[52]\tvalid_0's l2: 1.85355e+09\n",
      "[53]\tvalid_0's l2: 1.85511e+09\n",
      "[54]\tvalid_0's l2: 1.85607e+09\n",
      "[55]\tvalid_0's l2: 1.85816e+09\n",
      "[56]\tvalid_0's l2: 1.85979e+09\n",
      "[57]\tvalid_0's l2: 1.8581e+09\n",
      "[58]\tvalid_0's l2: 1.85872e+09\n",
      "[59]\tvalid_0's l2: 1.85838e+09\n",
      "[60]\tvalid_0's l2: 1.85821e+09\n",
      "[61]\tvalid_0's l2: 1.8559e+09\n",
      "[62]\tvalid_0's l2: 1.85582e+09\n",
      "[63]\tvalid_0's l2: 1.85439e+09\n",
      "[64]\tvalid_0's l2: 1.85232e+09\n",
      "[65]\tvalid_0's l2: 1.85379e+09\n",
      "[66]\tvalid_0's l2: 1.8534e+09\n",
      "[67]\tvalid_0's l2: 1.8528e+09\n",
      "[68]\tvalid_0's l2: 1.85292e+09\n",
      "[69]\tvalid_0's l2: 1.85219e+09\n",
      "[70]\tvalid_0's l2: 1.85329e+09\n",
      "[71]\tvalid_0's l2: 1.85284e+09\n",
      "[72]\tvalid_0's l2: 1.85312e+09\n",
      "[73]\tvalid_0's l2: 1.85344e+09\n",
      "[74]\tvalid_0's l2: 1.85269e+09\n",
      "[75]\tvalid_0's l2: 1.85434e+09\n",
      "[76]\tvalid_0's l2: 1.85624e+09\n",
      "[77]\tvalid_0's l2: 1.85758e+09\n",
      "[78]\tvalid_0's l2: 1.85633e+09\n",
      "[79]\tvalid_0's l2: 1.85581e+09\n",
      "[80]\tvalid_0's l2: 1.85579e+09\n",
      "[81]\tvalid_0's l2: 1.85843e+09\n",
      "[82]\tvalid_0's l2: 1.85841e+09\n",
      "[83]\tvalid_0's l2: 1.85817e+09\n",
      "[84]\tvalid_0's l2: 1.85878e+09\n",
      "[85]\tvalid_0's l2: 1.86137e+09\n",
      "[86]\tvalid_0's l2: 1.86169e+09\n",
      "[87]\tvalid_0's l2: 1.86173e+09\n",
      "[88]\tvalid_0's l2: 1.86191e+09\n",
      "[89]\tvalid_0's l2: 1.86184e+09\n",
      "[90]\tvalid_0's l2: 1.86139e+09\n",
      "[91]\tvalid_0's l2: 1.86695e+09\n",
      "[92]\tvalid_0's l2: 1.86743e+09\n",
      "[93]\tvalid_0's l2: 1.86802e+09\n",
      "[94]\tvalid_0's l2: 1.86828e+09\n",
      "[95]\tvalid_0's l2: 1.86722e+09\n",
      "[96]\tvalid_0's l2: 1.86863e+09\n",
      "[97]\tvalid_0's l2: 1.86864e+09\n",
      "[98]\tvalid_0's l2: 1.8677e+09\n",
      "[99]\tvalid_0's l2: 1.86764e+09\n",
      "[100]\tvalid_0's l2: 1.86746e+09\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 10.5 µs\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 8.82 µs\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 9.06 µs\n",
      "Validation RMSE: 43214.14765315515\n",
      "Test RMSE: 45713.30731611534\n",
      "Validation MAPE: 0.35724360420106843\n",
      "Test MAPE: 0.3326570151748478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arinkazam/.local/lib/python3.10/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import sklearn\n",
    "\n",
    "cat_features_train = X_train.dtypes == \"object\"\n",
    "transformer = make_column_transformer(\n",
    "    (OneHotEncoder(handle_unknown = 'ignore'), cat_features_train),\n",
    "    remainder='passthrough')\n",
    "transformed_train = transformer.fit_transform(X_train).toarray()\n",
    "X_train_l = pd.DataFrame(\n",
    "    transformed_train, \n",
    "    columns=transformer.get_feature_names_out())\n",
    "\n",
    "cat_features_val = X_val.dtypes == \"object\"\n",
    "\n",
    "transformed_val = transformer.transform(X_val).toarray()\n",
    "X_val_l = pd.DataFrame(\n",
    "    transformed_val, \n",
    "    columns=transformer.get_feature_names_out())\n",
    "\n",
    "cat_features_test = X_test.dtypes == \"object\"\n",
    "transformed_test = transformer.transform(X_test).toarray()\n",
    "X_test_l = pd.DataFrame(\n",
    "    transformed_test, \n",
    "    columns=transformer.get_feature_names_out())\n",
    "\n",
    "train_dataset = lgb.Dataset(X_train_l, y_train, feature_name=X_train_l.columns.values.tolist())\n",
    "val_dataset = lgb.Dataset(X_val_l, y_val, feature_name=X_val_l.columns.values.tolist())\n",
    "test_dataset = lgb.Dataset(X_test_l, y_test, feature_name=X_test_l.columns.values.tolist())\n",
    "\n",
    "%time\n",
    "booster = lgb.train({\"objective\": \"regression\", \"num_iterations\": 100, \"learning_rate\": 0.1, \"max_depth\": 20},\n",
    "                        train_set=train_dataset, valid_sets=(val_dataset,),\n",
    "                        num_boost_round=10)\n",
    "%time\n",
    "train_preds = booster.predict(X_train_l)\n",
    "%time\n",
    "val_preds = booster.predict(X_val_l)\n",
    "%time\n",
    "test_preds = booster.predict(X_test_l)\n",
    "\n",
    "print(f\"Validation RMSE: {mean_squared_error(y_val, val_preds, squared=False)}\")\n",
    "print(f\"Test RMSE: {mean_squared_error(y_test, test_preds, squared=False)}\")\n",
    "print(f\"Validation MAPE: {mean_absolute_percentage_error(y_val, val_preds)}\")\n",
    "print(f\"Test MAPE: {mean_absolute_percentage_error(y_test, test_preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c56d873",
   "metadata": {},
   "source": [
    "## Задание 10 (0.5 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2c1148",
   "metadata": {},
   "source": [
    "Сделайте выводы про модели бустинга, какая из моделей показала лучший результат по качеству, скорости обучения и скорости предсказания"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882cf3e6",
   "metadata": {},
   "source": [
    "### Ответ:\n",
    "Из двух моделей, которые я использовала (lightgbm, xgboost) лучше себя показал lightgbm, он и обучается быстрее, и качество выдал выше."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
